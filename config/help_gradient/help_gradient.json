{
  "Description": "The gradient descent algorithm is an optimization technique used to minimize a function by iteratively moving towards the steepest descent direction. The idea is to find the minimum of a function by updating the parameters (or variables) of the function in the opposite direction of the gradient, which points in the direction of the steepest increase. The algorithm continues updating the parameters until it converges to a local minimum or satisfies a stopping condition.",
  "Steps": "Initialize the variables with starting values. Calculate the gradient of the function at the current point. Update the variables by moving in the opposite direction of the gradient multiplied by learning rate. Repeat until the parameters converge to a minimum or the stopping condition is met.",
  "No. independent variables": "Number of independent variables used in the function to be minimized.",
  "Learning rate": "Controls the step size during the optimization process. It determines how much the model adjusts its parameters in response to the gradient. ",
  "Max iter": "Specifies the maximum number of iterations the algorithm will run for.",
  "Convergence": "Refers to the difference between values in two consecutive iterations that is used as a criterion to determine whether the optimal solution has been reached.",
  "Formula": "Represents a mathematical expression or function to be minimized using the gradient descent algorithm."
}